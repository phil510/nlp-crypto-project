{
  "cells":[
    {
      "cell_type":"code",
      "source":[
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModelForSequenceClassification\n",
        ")"
      ],
      "execution_count":1,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "DATA_DIR = '\/data\/workspace_files\/Twitter Datasets'\n",
        "OUTPUT_DIR = '\/data\/workspace_files\/RoBERTa Sentiment Scores'"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "### RoBERTa Sentiment Scoring\n",
        "\n",
        "In this notebook, we determine the sentiment score for each tweet using the pretrained RoBERTa model. Running a forward pass through the model for more than 5.6 million tweets (approximately 800 thousand from the Top 100 dataset and approximately 4.8 million from the Random Tweet dataset), about 8 hours even with a GPU. "
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count":2,
      "outputs":[
        {
          "data":{
            "text\/plain":[
              "device(type='cpu')"
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def chunks(lst, n):\n",
        "    \"\"\"Chunk a list into parts of size n.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]"
      ],
      "execution_count":3,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "The `PreTrainedTextModel` class below is a wrapper class for the Transformers `AutoTokenizer` and `AutoModelForSequenceClassification` classes, and handles the calculation of a single sentiment score from the softmax output from the model. "
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "class PreTrainedTextModel():\n",
        "    def __init__(self, model_name, **kwargs):\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
        "                                                                        **kwargs)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def predict(self, text, batch_size=32, device=None):\n",
        "        # set model up for prediction\n",
        "        self.model.eval()\n",
        "        if device:\n",
        "            self.model.to(device)\n",
        "\n",
        "        # predicting large amounts of text at once leads to memory errors\n",
        "        # we chunk the list of text and predict in batches\n",
        "        # additionally, torch.no_grad() stops gradient accumulation which can \n",
        "        # take up a lot of memory\n",
        "        predictions = []\n",
        "        with torch.no_grad():\n",
        "            for chunk in chunks(text, batch_size):\n",
        "                encodings = self.tokenizer(chunk, padding=True, truncation=True, \n",
        "                                        max_length=512, return_tensors='pt')\n",
        "\n",
        "                # move model inputs to device\n",
        "                if device:\n",
        "                    encodings.to(device)\n",
        "\n",
        "                output = self.model(**encodings)\n",
        "                pred = F.softmax(output.logits, dim=-1)\n",
        "\n",
        "                # label 0 is negative, label 1 is positive\n",
        "                # pulling the positive score ensures that positive sentiment is \n",
        "                # close to 1.0 and negative sentiment is close to 0.0\n",
        "                predictions.append(pred.cpu().numpy())\n",
        "\n",
        "        predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "        # score is a weighted average using the output probabilities as weights\n",
        "        scores = np.zeros(predictions.shape)\n",
        "        scores[:, 0] = -np.ones(predictions.shape[0])  # first columns is negative\n",
        "        scores[:, -1] = np.ones(predictions.shape[0])  # last column is positive\n",
        "\n",
        "        score = (predictions * scores).sum(axis=1)        \n",
        "\n",
        "        return predictions, score"
      ],
      "execution_count":4,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "First, we load the model and present its architecture. The model is a 12 layer transformer with a softmax output for predicting positive, negative, and neutral sentiment."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# model from: https:\/\/huggingface.co\/cardiffnlp\/twitter-roberta-base-sentiment?text=I+like+you.+I+love+you\n",
        "model_name = 'cardiffnlp\/twitter-roberta-base-sentiment'\n",
        "model = PreTrainedTextModel(model_name)\n",
        "model.model"
      ],
      "execution_count":8,
      "outputs":[
        {
          "data":{
            "text\/plain":[
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "First we predict the sentiment score for the Top 100 dataset."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "dtypes = {\n",
        "    'id': str,\n",
        "    'author_id': str,\n",
        "    'text': str,\n",
        "    'clean_text': str,\n",
        "    'dataset': str\n",
        "}\n",
        "data = pd.read_csv(os.path.join(DATA_DIR, 'twitter_dataset_top100.csv'),\n",
        "                   encoding='utf-8', dtype=dtypes, parse_dates=['created_at'])\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "data.info()"
      ],
      "execution_count":3,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 847540 entries, 0 to 847539\n",
            "Data columns (total 19 columns):\n",
            " #   Column         Non-Null Count   Dtype         \n",
            "---  ------         --------------   -----         \n",
            " 0   id             847540 non-null  object        \n",
            " 1   author_id      847540 non-null  object        \n",
            " 2   created_at     847540 non-null  datetime64[ns]\n",
            " 3   clean_text     847540 non-null  object        \n",
            " 4   btc_price      814694 non-null  float64       \n",
            " 5   eth_price      789475 non-null  float64       \n",
            " 6   btc_ret_+0.5h  814263 non-null  float64       \n",
            " 7   eth_ret_+0.5h  789035 non-null  float64       \n",
            " 8   btc_ret_+1h    813823 non-null  float64       \n",
            " 9   eth_ret_+1h    788593 non-null  float64       \n",
            " 10  btc_ret_+3h    810518 non-null  float64       \n",
            " 11  eth_ret_+3h    785292 non-null  float64       \n",
            " 12  btc_ret_+8h    805402 non-null  float64       \n",
            " 13  eth_ret_+8h    780172 non-null  float64       \n",
            " 14  btc_ret_+12h   801252 non-null  float64       \n",
            " 15  eth_ret_+12h   776030 non-null  float64       \n",
            " 16  btc_ret_+24h   794080 non-null  float64       \n",
            " 17  eth_ret_+24h   768857 non-null  float64       \n",
            " 18  dataset        847540 non-null  object        \n",
            "dtypes: datetime64[ns](1), float64(14), object(4)\n",
            "memory usage: 122.9+ MB\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "cols = ['id', 'created_at', 'clean_text']\n",
        "output_df = data.loc[:, cols]\n",
        "\n",
        "# takes ~1 hour for ~850K tweets\n",
        "text = data['clean_text'].to_list()\n",
        "pred, score = model.predict(text, device=device)\n",
        "\n",
        "output_df['score'] = score"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "output_df.info()"
      ],
      "execution_count":9,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 847540 entries, 0 to 847539\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count   Dtype         \n",
            "---  ------      --------------   -----         \n",
            " 0   id          847540 non-null  object        \n",
            " 1   created_at  847540 non-null  datetime64[ns]\n",
            " 2   clean_text  847540 non-null  object        \n",
            " 3   score       847540 non-null  float64       \n",
            "dtypes: datetime64[ns](1), float64(1), object(2)\n",
            "memory usage: 25.9+ MB\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "output_df.to_csv(os.path.join(OUTPUT_DIR), 'scores_twitter_roberta_pretrained_top100.csv')"
      ],
      "execution_count":10,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "Nest, we predict sentiment scores for the Random Tweet dataset. Because this is a much larger dataset and running the model for all 4.8 million tweets takes approximately 7 hours, we do predictions in batches of 500 thousand, saving the batches as we go. Once all tweets have a sentiment score, we combine the batch files into a single dataset. "
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "dtypes = {\n",
        "    'id': str,\n",
        "    'author_id': str,\n",
        "    'text': str,\n",
        "    'clean_text': str,\n",
        "    'dataset': str\n",
        "}\n",
        "data = pd.read_csv('\/data\/workspace_files\/twitter_dataset_random.csv',\n",
        "                   encoding='utf-8', dtype=dtypes, parse_dates=['created_at'])\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "data.info()"
      ],
      "execution_count":5,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4826627 entries, 0 to 4826626\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Dtype         \n",
            "---  ------      -----         \n",
            " 0   id          object        \n",
            " 1   author_id   object        \n",
            " 2   created_at  datetime64[ns]\n",
            " 3   clean_text  object        \n",
            "dtypes: datetime64[ns](1), object(3)\n",
            "memory usage: 147.3+ MB\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# we run this in chunks of 500k as predicting takes a very long time \n",
        "# so saving periodically is really important\n",
        "start_pos = 0\n",
        "chunk_size = 500000\n",
        "cols = ['id', 'created_at', 'clean_text']\n",
        "use_data = data.iloc[start_pos:, :]\n",
        "\n",
        "for i, chunk_df in enumerate(chunks(use_data, chunk_size)):\n",
        "    output_df = chunk_df.loc[:, cols].copy()\n",
        "    start = output_df.index.min()\n",
        "    end = output_df.index.max()\n",
        "\n",
        "    # takes ~1 hour for ~850K tweets\n",
        "    text = output_df['clean_text'].to_list()\n",
        "    pred, score = model.predict(text, device=device)\n",
        "\n",
        "    output_df['score'] = score\n",
        "\n",
        "    print('\\nChunk {:,} to {:,}'.format(start, end))\n",
        "    print(output_df.info())\n",
        "    \n",
        "    output_file = 'scores_twitter_roberta_pretrained_random_{}.csv'.format(end + 1)\n",
        "    output_df.to_csv(os.path.join(OUTPUT_DIR, output_file), index=False)"
      ],
      "execution_count":8,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "\n",
            "Chunk 0 to 499,999\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500000 entries, 0 to 499999\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count   Dtype         \n",
            "---  ------      --------------   -----         \n",
            " 0   id          500000 non-null  object        \n",
            " 1   created_at  500000 non-null  datetime64[ns]\n",
            " 2   clean_text  500000 non-null  object        \n",
            " 3   score       500000 non-null  float64       \n",
            "dtypes: datetime64[ns](1), float64(1), object(2)\n",
            "memory usage: 15.3+ MB\n",
            "None\n",
            "\n",
            "Chunk 500,000 to 999,999\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500000 entries, 500000 to 999999\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count   Dtype         \n",
            "---  ------      --------------   -----         \n",
            " 0   id          500000 non-null  object        \n",
            " 1   created_at  500000 non-null  datetime64[ns]\n",
            " 2   clean_text  500000 non-null  object        \n",
            " 3   score       500000 non-null  float64       \n",
            "dtypes: datetime64[ns](1), float64(1), object(2)\n",
            "memory usage: 15.3+ MB\n",
            "None\n",
            "\n",
            "Chunk 1,000,000 to 1,499,999\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500000 entries, 1000000 to 1499999\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count   Dtype         \n",
            "---  ------      --------------   -----         \n",
            " 0   id          500000 non-null  object        \n",
            " 1   created_at  500000 non-null  datetime64[ns]\n",
            " 2   clean_text  500000 non-null  object        \n",
            " 3   score       500000 non-null  float64       \n",
            "dtypes: datetime64[ns](1), float64(1), object(2)\n",
            "memory usage: 15.3+ MB\n",
            "None\n",
            "\n",
            "Chunk 1,500,000 to 1,999,999\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500000 entries, 1500000 to 1999999\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count   Dtype         \n",
            "---  ------      --------------   -----         \n",
            " 0   id          500000 non-null  object        \n",
            " 1   created_at  500000 non-null  datetime64[ns]\n",
            " 2   clean_text  500000 non-null  object        \n",
            " 3   score       500000 non-null  float64       \n",
            "dtypes: datetime64[ns](1), float64(1), object(2)\n",
            "memory usage: 15.3+ MB\n",
            "None\n",
            "\n",
            "Chunk 2,000,000 to 2,499,999\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500000 entries, 2000000 to 2499999\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count   Dtype         \n",
            "---  ------      --------------   -----         \n",
            " 0   id          500000 non-null  object        \n",
            " 1   created_at  500000 non-null  datetime64[ns]\n",
            " 2   clean_text  500000 non-null  object        \n",
            " 3   score       500000 non-null  float64       \n",
            "dtypes: datetime64[ns](1), float64(1), object(2)\n",
            "memory usage: 15.3+ MB\n",
            "None\n",
            "\n",
            "Chunk 2,500,000 to 2,999,999\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500000 entries, 2500000 to 2999999\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count   Dtype         \n",
            "---  ------      --------------   -----         \n",
            " 0   id          500000 non-null  object        \n",
            " 1   created_at  500000 non-null  datetime64[ns]\n",
            " 2   clean_text  500000 non-null  object        \n",
            " 3   score       500000 non-null  float64       \n",
            "dtypes: datetime64[ns](1), float64(1), object(2)\n",
            "memory usage: 15.3+ MB\n",
            "None\n",
            "\n",
            "Chunk 3,000,000 to 3,499,999\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500000 entries, 3000000 to 3499999\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count   Dtype         \n",
            "---  ------      --------------   -----         \n",
            " 0   id          500000 non-null  object        \n",
            " 1   created_at  500000 non-null  datetime64[ns]\n",
            " 2   clean_text  500000 non-null  object        \n",
            " 3   score       500000 non-null  float64       \n",
            "dtypes: datetime64[ns](1), float64(1), object(2)\n",
            "memory usage: 15.3+ MB\n",
            "None\n",
            "\n",
            "Chunk 3,500,000 to 3,999,999\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500000 entries, 3500000 to 3999999\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count   Dtype         \n",
            "---  ------      --------------   -----         \n",
            " 0   id          500000 non-null  object        \n",
            " 1   created_at  500000 non-null  datetime64[ns]\n",
            " 2   clean_text  500000 non-null  object        \n",
            " 3   score       500000 non-null  float64       \n",
            "dtypes: datetime64[ns](1), float64(1), object(2)\n",
            "memory usage: 15.3+ MB\n",
            "None\n",
            "\n",
            "Chunk 4,000,000 to 4,499,999\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500000 entries, 4000000 to 4499999\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count   Dtype         \n",
            "---  ------      --------------   -----         \n",
            " 0   id          500000 non-null  object        \n",
            " 1   created_at  500000 non-null  datetime64[ns]\n",
            " 2   clean_text  500000 non-null  object        \n",
            " 3   score       500000 non-null  float64       \n",
            "dtypes: datetime64[ns](1), float64(1), object(2)\n",
            "memory usage: 15.3+ MB\n",
            "None\n",
            "\n",
            "Chunk 4,500,000 to 4,826,626\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 326627 entries, 4500000 to 4826626\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count   Dtype         \n",
            "---  ------      --------------   -----         \n",
            " 0   id          326627 non-null  object        \n",
            " 1   created_at  326627 non-null  datetime64[ns]\n",
            " 2   clean_text  326627 non-null  object        \n",
            " 3   score       326627 non-null  float64       \n",
            "dtypes: datetime64[ns](1), float64(1), object(2)\n",
            "memory usage: 10.0+ MB\n",
            "None\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# collect and save the different chunks into a single file\n",
        "files = os.listdir(OUTPUT_DIR)\n",
        "\n",
        "def sorter(x):\n",
        "    try:\n",
        "        return int(re.findall('_(\\d+)[.]', x)[0])\n",
        "    except IndexError:\n",
        "        return np.inf\n",
        "files.sort(key=sorter)\n",
        "\n",
        "data = pd.DataFrame()\n",
        "for f in files:\n",
        "    if not re.search('_random_[0-9]+', f):\n",
        "        continue\n",
        "\n",
        "    df = pd.read_csv(os.path.join(OUTPUT_DIR, f), \n",
        "                     parse_dates=['created_at'], \n",
        "                     dtype={'id': str})\n",
        "    data = data.append(df, ignore_index=True)"
      ],
      "execution_count":6,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "data.info()"
      ],
      "execution_count":7,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4826627 entries, 0 to 4826626\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Dtype         \n",
            "---  ------      -----         \n",
            " 0   id          object        \n",
            " 1   created_at  datetime64[ns]\n",
            " 2   clean_text  object        \n",
            " 3   score       float64       \n",
            "dtypes: datetime64[ns](1), float64(1), object(2)\n",
            "memory usage: 147.3+ MB\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "output_file = 'scores_twitter_roberta_pretrained_random_all.csv'\n",
        "data.to_csv(os.path.join(OUTPUT_DIR, output_file), index=False)"
      ],
      "execution_count":10,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    }
  ],
  "metadata":{
    
  },
  "nbformat":4,
  "nbformat_minor":0
}